{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning**\n",
    "\n",
    "It is a RL Algorithm. It is model free algo that is used to calculate optimal action-selection policy for finite MDPs (Markov Decision Process).\n",
    "By model free, it means that it doesn't need to know the enviornment.\n",
    "\n",
    "As stated:\n",
    "*This means that the agent does not need an understanding of the environment (model-free) to learn and act, and keeps track of the optimal actions to take in given states that it observes- best used in environments with a finite set of steps, states, and actions to take (finite MDP). Q in this case stands for \"Quality\"*\n",
    "\n",
    "**Some Concepts:**\n",
    "\n",
    "Value-based learning: Q-learning estimates the value with each state to do decision-making.\n",
    "\n",
    "State-action pairs: Learning is made from state values paired with specific actions in each space, instead of only from state values.\n",
    "\n",
    "Iterative Q-Values: Q-values are updated continously with each experience for learning refinement/improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solving this maze problem using Q-learning.**\n",
    "\n",
    "<img src=\"image.png\" height=\"200\" />\n",
    "\n",
    "Teaching the agent to reach the goal (G) using Q-learning and avoiding obstacle in 3x3 grid.\n",
    "\n",
    "**Output**\n",
    "\n",
    "Output of this code will be a resultant Q-table, that will hold the q-values for each state-action pair. Q-values are expected utility acheived by the agent for each state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Define the grid world\n",
    "GRID_SIZE = 3\n",
    "START = (0, 0)\n",
    "GOAL = (2, 2)\n",
    "OBSTACLE = (1, 1)\n",
    "\n",
    "# Define actions\n",
    "ACTIONS = [\n",
    "    (-1, 0),  # up\n",
    "    (0, 1),   # right\n",
    "    (1, 0),   # down\n",
    "    (0, -1)   # left\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and \n",
    "            0 <= state[1] < GRID_SIZE and \n",
    "            state != OBSTACLE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
