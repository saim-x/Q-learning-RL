{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning**\n",
    "\n",
    "It is a RL Algorithm. It is model free algo that is used to calculate optimal action-selection policy for finite MDPs (Markov Decision Process).\n",
    "By model free, it means that it doesn't need to know the enviornment.\n",
    "\n",
    "As stated:\n",
    "*This means that the agent does not need an understanding of the environment (model-free) to learn and act, and keeps track of the optimal actions to take in given states that it observes- best used in environments with a finite set of steps, states, and actions to take (finite MDP). Q in this case stands for \"Quality\"*\n",
    "\n",
    "**Some Concepts:**\n",
    "\n",
    "Value-based learning: Q-learning estimates the value with each state to do decision-making.\n",
    "\n",
    "State-action pairs: Learning is made from state values paired with specific actions in each space, instead of only from state values.\n",
    "\n",
    "Iterative Q-Values: Q-values are updated continously with each experience for learning refinement/improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solving this maze problem using Q-learning.**\n",
    "\n",
    "<img src=\"image.png\" height=\"200\" />\n",
    "\n",
    "Teaching the agent to reach the goal (G) using Q-learning and avoiding obstacle in 3x3 grid.\n",
    "\n",
    "**Output**\n",
    "\n",
    "Output of this code will be a resultant Q-table, that will hold the q-values for each state-action pair. Q-values are expected utility acheived by the agent for each state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Define the grid world\n",
    "GRID_SIZE = 3\n",
    "START = (0, 0)\n",
    "GOAL = (2, 2)\n",
    "OBSTACLE = (1, 1)\n",
    "\n",
    "# Define actions\n",
    "ACTIONS = [\n",
    "    (-1, 0),  # up\n",
    "    (0, 1),   # right\n",
    "    (1, 0),   # down\n",
    "    (0, -1)   # left\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and \n",
    "            0 <= state[1] < GRID_SIZE and \n",
    "            state != OBSTACLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple[int, int]:\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    return next_state if is_valid_state(next_state) else state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Defining Q-Learning parameters**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Epsilon (ε) - Exploration Rate**  \n",
    "Think of it as **the agent's curiosity**.  \n",
    "\n",
    "- **What it does**: Determines how often the agent will try something new (explore) vs. sticking to what it knows works (exploit).\n",
    "- **Example**:  \n",
    "  - If **ε = 0.3**, the agent will explore (try random moves) 30% of the time and exploit (choose the best known action) 70% of the time.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Alpha (α) - Learning Rate**  \n",
    "This is **how much the agent listens to new advice**.  \n",
    "\n",
    "- **What it does**: Decides how much weight the agent gives to new information compared to what it already knows.\n",
    "- **Example**:  \n",
    "  - If **α = 0.3**, the agent learns 30% from new experiences and keeps 70% of its old knowledge.\n",
    "Memory hook**: *\"How eager is the agent to learn new tricks?\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Gamma (γ) - Discount Factor**  \n",
    "This is **the agent’s patience**.  \n",
    "\n",
    "- **What it does**: Balances the importance of immediate vs. future rewards.\n",
    "- **Example**:  \n",
    "  - If **γ = 0.99**, future rewards are valued at 99% of their original worth.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Episodes**  \n",
    "These are **training rounds** for the agent.  \n",
    "\n",
    "- **What it does**: Each episode is like a practice game where the agent starts fresh and tries to learn from scratch.\n",
    "- **Example**:  \n",
    "  - If there are **10,000 episodes**, the agent gets 10,000 chances to improve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.3\n",
    "ALPHA = 0.3\n",
    "GAMMA = 0.99\n",
    "EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif next_state == OBSTACLE or next_state == state:  # Penalize hitting walls or obstacle\n",
    "        return -10\n",
    "    else:\n",
    "        return -1  # Small penalty for each step to encourage shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    If the random number is less than EPSILON, the function returns a random action from the ACTIONS list, and if the random number is greater than or equal to EPSILON, the function chooses the action with the highest Q-value for the current state.\n",
    "\n",
    "    Balancing exploration and exploitation ensures the agent doesn't always choose the best known action (which could lead to getting stuck in suboptimal solutions) or always choose randomly (which would prevent learning). This allows for continuous learning- even when the agent has learned a good policy, the occasional random actions allow it to potentially find even better solutions.\n",
    "    With an epsilon of 0.3, we will take a random action 30% of the time, and exploit the best known action 70% of the time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
